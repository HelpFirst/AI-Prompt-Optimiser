{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial prompt and output format\n",
    "initial_prompt = (\n",
    "    \"Please look for the following risk factor: Is the client at risk of self-harm? For instance, do they mention suicidal thoughts or ideation? Do they imply they might do physical damage to themselves or to property? Do they reference wanting to 'end it all or say it's 'not worth living'? Please output: 1. risk_type: // suicidality, 2. risk_output: // 'risk present' : this means there is evidence this risk is present in the case 'risk not present' : there is evidence the risk is NOT present or there is no evidence whether the case contains that risk or not. (If in doubt, it is better to err on the side of caution and say 'risk present') 3. explanation: // State words/terms that indicate the reason the risk_output was chosen. Be brief in your explanation. State facts found in the text, do not infer. E.g. 'Client expressed suicidal ideation'. Leave blank for 'risk not present.'\"\n",
    ")\n",
    "\n",
    "output_format_prompt = (\n",
    "    \"Output should be STRICT JSON, containing: dictionary containing the type of risk with their output and explanation, formatted like this: {'risk_type': 'suicidality', 'risk_output': str, 'explanation': str}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_prompt = (\n",
    "#         \"You are a sentiment analysis classifier. Determine whether the provided text expresses a positive sentiment. \"\n",
    "#         \"Respond with '1' if it is positive, or '0' if it is negative.\"\n",
    "#     )\n",
    "\n",
    "# # Output format prompt\n",
    "# output_format_prompt = (\n",
    "#     \"You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for negative. \"\n",
    "#     \"Do NOT include any additional text or explanation.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output schema\n",
    "output_schema = {\n",
    "    'key_to_extract': 'risk_output',\n",
    "    'value_mapping': {\n",
    "        'risk_present': 1,\n",
    "        'risk_not_present': 0\n",
    "    },\n",
    "    'regex_pattern': r'\"risk_output\":\\s*\"(.*?)\"'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_schema = {\n",
    "#     'key_to_extract': None,  # Set to None for direct output\n",
    "#     'value_mapping': None,   # Set to None for direct mapping\n",
    "#     'regex_pattern': r'^(0|1)$'  # Match the entire output for binary classification\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of optimization iterations\n",
    "iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model providers and models for evaluation and optimization\n",
    "eval_provider = \"ollama\"\n",
    "eval_model = \"llama3.1\"\n",
    "optim_provider = \"ollama\"\n",
    "optim_model = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing review data for evaluation\n",
    "eval_datapath = \"reviews.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to sys.path\n",
    "# Use getcwd() to get the current working directory for Jupyter notebooks\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from src.iterative_prompt_optimization import optimize_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation data shape: (10, 2)\n",
      "                                                text  label\n",
      "0  Was this based on a comic-book? A video-game? ...      1\n",
      "1  If you ask me the first one was really better ...      0\n",
      "2  When I was a kid, I loved \"Tiny Toons\". I espe...      1\n",
      "3  I hate guns and have never murdered anyone, bu...      0\n",
      "4  I do not have much to say than this is a great...      1\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "eval_data = pd.read_csv(eval_datapath, encoding='ISO-8859-1', usecols=['Text', 'Sentiment'])\n",
    "eval_data.columns = ['text', 'label']\n",
    "# Randomly select 50 positive and 50 negative samples\n",
    "eval_data = (\n",
    "    eval_data.groupby('label')\n",
    "    .apply(lambda x: x.sample(n=5, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "# Shuffle the DataFrame randomly\n",
    "eval_data = eval_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Evaluation data shape: {eval_data.shape}\")\n",
    "print(eval_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected evaluation provider: ollama\n",
      "Selected evaluation model: llama3.1\n",
      "Selected optimization provider: ollama\n",
      "Selected optimization model: llama3.1\n",
      "Estimated token usage: 112440\n",
      "Estimated cost: $0 API Costs - Running on Local Hardware\n",
      "\n",
      "Do you want to proceed with the optimization? (Y/N): \n",
      "Iteration 1/3\n",
      "Processing text 1/10\n",
      "Invalid output: Extracted value 'not present' not found in value_mapping\n",
      "Prediction 1/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 2/10\n",
      "JSON Decode Error: Unable to parse extracted JSON\n",
      "Prediction 2/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 3/10\n",
      "Prediction 3/10: 0 | Ground Truth: 1 ❌ (FN)\n",
      "Processing text 4/10\n",
      "Prediction 4/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 5/10\n",
      "Prediction 5/10: 0 | Ground Truth: 1 ❌ (FN)\n",
      "Processing text 6/10\n",
      "Prediction 6/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 7/10\n",
      "Prediction 7/10: 0 | Ground Truth: 1 ❌ (FN)\n",
      "Processing text 8/10\n",
      "Prediction 8/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 9/10\n",
      "Prediction 9/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 10/10\n",
      "Prediction 10/10: 0 | Ground Truth: 1 ❌ (FN)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 1</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.0000 │\n",
       "│ Recall              │ 0.0000 │\n",
       "│ Accuracy            │ 0.5000 │\n",
       "│ F1-score            │ 0.0000 │\n",
       "│ Invalid Predictions │      2 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 1\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.0000 │\n",
       "│ Recall              │ 0.0000 │\n",
       "│ Accuracy            │ 0.5000 │\n",
       "│ F1-score            │ 0.0000 │\n",
       "│ Invalid Predictions │      2 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassifications...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">╭──────────────────────────────────────── Analysis of Misclassifications ─────────────────────────────────────────╮</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Analysis**                                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> The model incorrectly classified a total of 5 texts: 3 as positive (when they were actually negative) and 2 as  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> negative (when they were actually positive).                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **False Positives (Incorrectly Classified as Positive)**                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Example**: \"I loved 'Tiny Toons'... A few years later, my friend had the video. And I figured I'd watch it <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> for the good old days. I was still on the floors laughing.\" (Actual classification: Negative)                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * This text appears to be an enthusiastic endorsement of a cartoon, but upon closer inspection, the     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> tone is actually nostalgic and humorous, with the author expressing a desire to relive their childhood          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> memories.                                                                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * The model likely misclassified this text due to the presence of positive sentiment words (\"loved,\"    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"laughing\") and the lack of explicit negative language.                                                         <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Example**: \"I personally think this is what was needed, a fight to end it all... I loved every second of   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> this movie.\" (Actual classification: Negative)                                                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * This text is actually an argument in favor of the film's ending, despite some critics' opinions that  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> it was too action-oriented.                                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * The model may have misclassified this text due to the presence of words like \"loved\" and \"every       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> second,\" which convey a positive sentiment, without fully understanding the context and nuance of the author's  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> opinion.                                                                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3. **Example**: \"I've seen this movie 3 times &amp; I've liked it every time.\" (Actual classification: Negative)    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * This text is actually an endorsement of a TV movie about serial killer Andrei Chikatilo, despite its  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> dark subject matter.                                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * The model likely misclassified this text due to the presence of positive sentiment words (\"liked,\"    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"every time\") and the lack of explicit negative language.                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **False Negatives (Incorrectly Classified as Negative)**                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Example**: \"This is one of the funniest cartoons I have ever seen.\" (Actual classification: Positive)      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * This text is a clear endorsement of a cartoon, using positive sentiment words like \"funniest\" and     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"ever.\"                                                                                                         <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * The model likely misclassified this text due to the lack of explicit negative language or critical    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> tone.                                                                                                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Example**: \"I have rated this film at the highest possible rating.\" (Actual classification: Positive)      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * This text is a clear endorsement of a short film, using positive sentiment words like \"highest\" and   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"excellent quality.\"                                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * The model likely misclassified this text due to the lack of explicit negative language or critical    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> tone.                                                                                                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Insights**                                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> The errors in classification suggest that the prompt may be too sensitive to positive sentiment words, without  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> fully considering the context and nuance of the text. Additionally, the prompt may not be adequately trained on <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> texts with complex opinions or nuanced sentiments.                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Strategies for Improvement**                                                                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Add more training data**: Include a diverse set of texts with complex opinions and nuanced sentiments to   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> improve the model's understanding of subtle differences in classification.                                      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Implement sentiment analysis techniques**: Use techniques like sentiment intensity, emotional tone, and    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> context-aware sentiment analysis to better understand the nuances of text sentiment.                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3. **Incorporate more features**: Incorporate additional features, such as topic modeling, entity recognition,  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> or aspect-based sentiment analysis, to provide a more comprehensive understanding of text content.              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 4. **Adjust the prompt's sensitivity**: Adjust the prompt's sensitivity to balance false positives and false    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> negatives by considering both explicit and implicit negative language.                                          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 5. **Use ensemble methods**: Combine multiple models with different architectures or training data to improve   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> overall performance and reduce errors.                                                                          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> By implementing these strategies, the classification prompt can better recognize nuances in text sentiment,     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> reducing both false positives and false negatives and improving overall accuracy.                               <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m╭─\u001b[0m\u001b[1m───────────────────────────────────────\u001b[0m\u001b[1m Analysis of Misclassifications \u001b[0m\u001b[1m────────────────────────────────────────\u001b[0m\u001b[1m─╮\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Analysis**                                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m The model incorrectly classified a total of 5 texts: 3 as positive (when they were actually negative) and 2 as  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m negative (when they were actually positive).                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **False Positives (Incorrectly Classified as Positive)**                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Example**: \"I loved 'Tiny Toons'... A few years later, my friend had the video. And I figured I'd watch it \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m for the good old days. I was still on the floors laughing.\" (Actual classification: Negative)                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * This text appears to be an enthusiastic endorsement of a cartoon, but upon closer inspection, the     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m tone is actually nostalgic and humorous, with the author expressing a desire to relive their childhood          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m memories.                                                                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * The model likely misclassified this text due to the presence of positive sentiment words (\"loved,\"    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"laughing\") and the lack of explicit negative language.                                                         \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Example**: \"I personally think this is what was needed, a fight to end it all... I loved every second of   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m this movie.\" (Actual classification: Negative)                                                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * This text is actually an argument in favor of the film's ending, despite some critics' opinions that  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m it was too action-oriented.                                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * The model may have misclassified this text due to the presence of words like \"loved\" and \"every       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m second,\" which convey a positive sentiment, without fully understanding the context and nuance of the author's  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m opinion.                                                                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3. **Example**: \"I've seen this movie 3 times & I've liked it every time.\" (Actual classification: Negative)    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * This text is actually an endorsement of a TV movie about serial killer Andrei Chikatilo, despite its  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m dark subject matter.                                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * The model likely misclassified this text due to the presence of positive sentiment words (\"liked,\"    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"every time\") and the lack of explicit negative language.                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **False Negatives (Incorrectly Classified as Negative)**                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Example**: \"This is one of the funniest cartoons I have ever seen.\" (Actual classification: Positive)      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * This text is a clear endorsement of a cartoon, using positive sentiment words like \"funniest\" and     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"ever.\"                                                                                                         \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * The model likely misclassified this text due to the lack of explicit negative language or critical    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m tone.                                                                                                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Example**: \"I have rated this film at the highest possible rating.\" (Actual classification: Positive)      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * This text is a clear endorsement of a short film, using positive sentiment words like \"highest\" and   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"excellent quality.\"                                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * The model likely misclassified this text due to the lack of explicit negative language or critical    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m tone.                                                                                                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Insights**                                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m The errors in classification suggest that the prompt may be too sensitive to positive sentiment words, without  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m fully considering the context and nuance of the text. Additionally, the prompt may not be adequately trained on \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m texts with complex opinions or nuanced sentiments.                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Strategies for Improvement**                                                                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Add more training data**: Include a diverse set of texts with complex opinions and nuanced sentiments to   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m improve the model's understanding of subtle differences in classification.                                      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Implement sentiment analysis techniques**: Use techniques like sentiment intensity, emotional tone, and    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m context-aware sentiment analysis to better understand the nuances of text sentiment.                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3. **Incorporate more features**: Incorporate additional features, such as topic modeling, entity recognition,  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m or aspect-based sentiment analysis, to provide a more comprehensive understanding of text content.              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 4. **Adjust the prompt's sensitivity**: Adjust the prompt's sensitivity to balance false positives and false    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m negatives by considering both explicit and implicit negative language.                                          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 5. **Use ensemble methods**: Combine multiple models with different architectures or training data to improve   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m overall performance and reduce errors.                                                                          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m By implementing these strategies, the classification prompt can better recognize nuances in text sentiment,     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m reducing both false positives and false negatives and improving overall accuracy.                               \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new prompt...\n",
      "\n",
      "Iteration 2/3\n",
      "Processing text 1/10\n",
      "Invalid output: Extracted value 'not present' not found in value_mapping\n",
      "Prediction 1/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 2/10\n",
      "Unable to find risk_output in the raw output\n",
      "Prediction 2/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 3/10\n",
      "JSON Decode Error: Unable to parse extracted JSON\n",
      "Prediction 3/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 4/10\n",
      "JSON Decode Error: Unable to parse extracted JSON\n",
      "Prediction 4/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 5/10\n",
      "Invalid output: Extracted value 'not present' not found in value_mapping\n",
      "Prediction 5/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 6/10\n",
      "Invalid output: Extracted value 'not present' not found in value_mapping\n",
      "Prediction 6/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 7/10\n",
      "Invalid output: Extracted value 'not present' not found in value_mapping\n",
      "Prediction 7/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 8/10\n",
      "Invalid output: Extracted value 'Not Present' not found in value_mapping\n",
      "Prediction 8/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 9/10\n",
      "Invalid output: Extracted value 'not present' not found in value_mapping\n",
      "Prediction 9/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 10/10\n",
      "Invalid output: Extracted value 'not present' not found in value_mapping\n",
      "Prediction 10/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 2</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.0000 │\n",
       "│ Recall              │ 0.0000 │\n",
       "│ Accuracy            │ 0.0000 │\n",
       "│ F1-score            │ 0.0000 │\n",
       "│ Invalid Predictions │     10 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 2\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.0000 │\n",
       "│ Recall              │ 0.0000 │\n",
       "│ Accuracy            │ 0.0000 │\n",
       "│ F1-score            │ 0.0000 │\n",
       "│ Invalid Predictions │     10 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassifications...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">╭──────────────────────────────────────── Analysis of Misclassifications ─────────────────────────────────────────╮</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> After analyzing the misclassified texts, I've identified specific examples from each set where the model made a <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> mistake and highlighted what elements of the text may have led to the incorrect classification.                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Negative (0) texts incorrectly classified as positive:**                                                      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Text:** \"I'm so excited for my upcoming vacation in Hawaii! The beaches are stunning, and I've heard great <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> things about the luaus.\"                                                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Incorrect classification:** Positive                                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Correct classification:** Should be Negative, as this text is not expressing any negative sentiment <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> or tone.                                                                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Insight:** This misclassification might be due to the presence of positive words like \"excited\" and <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"stunning,\" which overrode the model's ability to detect a lack of negativity.                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Text:** \"I've been trying to get into shape for my wedding, but it's been tough. I'm struggling with       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> motivation.\"                                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Incorrect classification:** Positive                                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Correct classification:** Should be Negative, as this text expresses frustration and                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> disappointment.                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Insight:** This misclassification might be due to the model's tendency to focus on words like       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"wedding\" and \"motivation,\" which have a positive connotation, rather than detecting the underlying negative    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> emotions.                                                                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Positive (1) texts incorrectly classified as negative:**                                                      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Text:** \"I just tried out my new cooking class and I'm absolutely loving it! The instructors are so        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> helpful.\"                                                                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Incorrect classification:** Negative                                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Correct classification:** Should be Positive, as this text expresses enthusiasm and satisfaction.   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Insight:** This misclassification might be due to the model's sensitivity to words like             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"struggling\" or \"difficulty,\" which can sometimes appear in positive texts (e.g., \"I'm struggling to choose     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> between two amazing options\"). In this case, the presence of \"absolutely loving it\" and \"helpful\" likely        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> overrode any negative connotations.                                                                             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Text:** \"I recently went on a meditation retreat and had an incredible experience. I feel more centered    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> than ever.\"                                                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Incorrect classification:** Negative                                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Correct classification:** Should be Positive, as this text expresses joy and well-being.            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * **Insight:** This misclassification might be due to the model's tendency to focus on words like       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"retreat,\" which can sometimes imply isolation or negative experiences. However, in this case, the presence of  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> positive words like \"incredible\" and \"centered\" likely overrode any potential negativity.                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Strategies to improve the classification prompt:**                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Emphasize nuance:** Incorporate more subtle language around sentiment, such as phrases that hint at        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> underlying emotions (e.g., \"I'm struggling,\" but also \"I'm excited\") or words that convey tone and attitude     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> (e.g., \"absolutely loving it\").                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Contextualize the text:** Consider adding context to the prompt, which can help disambiguate positive and  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> negative sentiment. For example, if a user mentions \"struggling\" in the context of a cooking class, this could  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> indicate enthusiasm rather than frustration.                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3. **Focus on tone markers:** Train the model to recognize words or phrases that convey strong emotions         <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> (positive or negative). Tone markers like \"absolutely,\" \"extremely,\" or \"I'm so grateful for\" can help          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> differentiate between positive and negative sentiment.                                                          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 4. **Use more robust sentiment analysis:** Consider incorporating additional sentiment analysis techniques,     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> such as aspect-based sentiment analysis, which can detect specific aspects of a text that convey sentiment      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> (e.g., the instructor's helpfulness in the cooking class example).                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 5. **Regularly retrain and fine-tune the model:** Continuously update the model with new data and refine its    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> performance using active learning methods, where human evaluators provide feedback on the most uncertain        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> examples.                                                                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> By implementing these strategies, you can improve the classification prompt to better recognize the nuances     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> that led to errors in sentiment analysis. This will help reduce both false positives (incorrectly classified as <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> positive) and false negatives (incorrectly classified as negative), leading to more accurate predictions for    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> your users.                                                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m╭─\u001b[0m\u001b[1m───────────────────────────────────────\u001b[0m\u001b[1m Analysis of Misclassifications \u001b[0m\u001b[1m────────────────────────────────────────\u001b[0m\u001b[1m─╮\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m After analyzing the misclassified texts, I've identified specific examples from each set where the model made a \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m mistake and highlighted what elements of the text may have led to the incorrect classification.                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Negative (0) texts incorrectly classified as positive:**                                                      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Text:** \"I'm so excited for my upcoming vacation in Hawaii! The beaches are stunning, and I've heard great \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m things about the luaus.\"                                                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Incorrect classification:** Positive                                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Correct classification:** Should be Negative, as this text is not expressing any negative sentiment \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m or tone.                                                                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Insight:** This misclassification might be due to the presence of positive words like \"excited\" and \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"stunning,\" which overrode the model's ability to detect a lack of negativity.                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Text:** \"I've been trying to get into shape for my wedding, but it's been tough. I'm struggling with       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m motivation.\"                                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Incorrect classification:** Positive                                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Correct classification:** Should be Negative, as this text expresses frustration and                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m disappointment.                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Insight:** This misclassification might be due to the model's tendency to focus on words like       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"wedding\" and \"motivation,\" which have a positive connotation, rather than detecting the underlying negative    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m emotions.                                                                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Positive (1) texts incorrectly classified as negative:**                                                      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Text:** \"I just tried out my new cooking class and I'm absolutely loving it! The instructors are so        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m helpful.\"                                                                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Incorrect classification:** Negative                                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Correct classification:** Should be Positive, as this text expresses enthusiasm and satisfaction.   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Insight:** This misclassification might be due to the model's sensitivity to words like             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"struggling\" or \"difficulty,\" which can sometimes appear in positive texts (e.g., \"I'm struggling to choose     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m between two amazing options\"). In this case, the presence of \"absolutely loving it\" and \"helpful\" likely        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m overrode any negative connotations.                                                                             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Text:** \"I recently went on a meditation retreat and had an incredible experience. I feel more centered    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m than ever.\"                                                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Incorrect classification:** Negative                                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Correct classification:** Should be Positive, as this text expresses joy and well-being.            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * **Insight:** This misclassification might be due to the model's tendency to focus on words like       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"retreat,\" which can sometimes imply isolation or negative experiences. However, in this case, the presence of  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m positive words like \"incredible\" and \"centered\" likely overrode any potential negativity.                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Strategies to improve the classification prompt:**                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Emphasize nuance:** Incorporate more subtle language around sentiment, such as phrases that hint at        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m underlying emotions (e.g., \"I'm struggling,\" but also \"I'm excited\") or words that convey tone and attitude     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m (e.g., \"absolutely loving it\").                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Contextualize the text:** Consider adding context to the prompt, which can help disambiguate positive and  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m negative sentiment. For example, if a user mentions \"struggling\" in the context of a cooking class, this could  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m indicate enthusiasm rather than frustration.                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3. **Focus on tone markers:** Train the model to recognize words or phrases that convey strong emotions         \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m (positive or negative). Tone markers like \"absolutely,\" \"extremely,\" or \"I'm so grateful for\" can help          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m differentiate between positive and negative sentiment.                                                          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 4. **Use more robust sentiment analysis:** Consider incorporating additional sentiment analysis techniques,     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m such as aspect-based sentiment analysis, which can detect specific aspects of a text that convey sentiment      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m (e.g., the instructor's helpfulness in the cooking class example).                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 5. **Regularly retrain and fine-tune the model:** Continuously update the model with new data and refine its    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m performance using active learning methods, where human evaluators provide feedback on the most uncertain        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m examples.                                                                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m By implementing these strategies, you can improve the classification prompt to better recognize the nuances     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m that led to errors in sentiment analysis. This will help reduce both false positives (incorrectly classified as \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m positive) and false negatives (incorrectly classified as negative), leading to more accurate predictions for    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m your users.                                                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new prompt...\n",
      "\n",
      "Iteration 3/3\n",
      "Processing text 1/10\n",
      "Unable to find risk_output in the raw output\n",
      "Prediction 1/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 2/10\n",
      "Unable to find risk_output in the raw output\n",
      "Prediction 2/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 3/10\n",
      "Invalid output: Extracted value 'None' not found in value_mapping\n",
      "Prediction 3/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 4/10\n",
      "JSON Decode Error: Unable to parse extracted JSON\n",
      "Prediction 4/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 5/10\n",
      "Invalid output: Extracted value 'Not found' not found in value_mapping\n",
      "Prediction 5/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 6/10\n",
      "JSON Decode Error: Unable to parse extracted JSON\n",
      "Prediction 6/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 7/10\n",
      "JSON Decode Error: Unable to parse extracted JSON\n",
      "Prediction 7/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n",
      "Processing text 8/10\n",
      "Invalid output: Extracted value 'low' not found in value_mapping\n",
      "Prediction 8/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 9/10\n",
      "Invalid output: Extracted value 'No risk present' not found in value_mapping\n",
      "Prediction 9/10: None | Ground Truth: 0 🛠️ (Invalid Output Format)\n",
      "Processing text 10/10\n",
      "JSON Decode Error: Unable to parse extracted JSON\n",
      "Prediction 10/10: None | Ground Truth: 1 🛠️ (Invalid Output Format)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 3</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.0000 │\n",
       "│ Recall              │ 0.0000 │\n",
       "│ Accuracy            │ 0.0000 │\n",
       "│ F1-score            │ 0.0000 │\n",
       "│ Invalid Predictions │     10 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 3\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.0000 │\n",
       "│ Recall              │ 0.0000 │\n",
       "│ Accuracy            │ 0.0000 │\n",
       "│ F1-score            │ 0.0000 │\n",
       "│ Invalid Predictions │     10 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Best Prompt:</span> │\n",
       "╰──────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────╮\n",
       "│ \u001b[1;32mBest Prompt:\u001b[0m │\n",
       "╰──────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Please look for the following risk factor: Is the client at risk of self-harm? For instance, do they mention \n",
       "suicidal thoughts or ideation? Do they imply they might do physical damage to themselves or to property? Do they \n",
       "reference wanting to <span style=\"color: #008000; text-decoration-color: #008000\">'end it all or say it'</span>s <span style=\"color: #008000; text-decoration-color: #008000\">'not worth living'</span>? Please output: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. risk_type: <span style=\"color: #800080; text-decoration-color: #800080\">//</span> suicidality, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. \n",
       "risk_output: <span style=\"color: #800080; text-decoration-color: #800080\">//</span> <span style=\"color: #008000; text-decoration-color: #008000\">'risk present'</span> : this means there is evidence this risk is present in the case <span style=\"color: #008000; text-decoration-color: #008000\">'risk not present'</span> :\n",
       "there is evidence the risk is NOT present or there is no evidence whether the case contains that risk or not. <span style=\"font-weight: bold\">(</span>If \n",
       "in doubt, it is better to err on the side of caution and say <span style=\"color: #008000; text-decoration-color: #008000\">'risk present'</span><span style=\"font-weight: bold\">)</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. explanation: <span style=\"color: #800080; text-decoration-color: #800080\">//</span> State words/terms \n",
       "that indicate the reason the risk_output was chosen. Be brief in your explanation. State facts found in the text, \n",
       "do not infer. E.g. <span style=\"color: #008000; text-decoration-color: #008000\">'Client expressed suicidal ideation'</span>. Leave blank for <span style=\"color: #008000; text-decoration-color: #008000\">'risk not present.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Please look for the following risk factor: Is the client at risk of self-harm? For instance, do they mention \n",
       "suicidal thoughts or ideation? Do they imply they might do physical damage to themselves or to property? Do they \n",
       "reference wanting to \u001b[32m'end it all or say it'\u001b[0ms \u001b[32m'not worth living'\u001b[0m? Please output: \u001b[1;36m1\u001b[0m. risk_type: \u001b[35m/\u001b[0m\u001b[35m/\u001b[0m suicidality, \u001b[1;36m2\u001b[0m. \n",
       "risk_output: \u001b[35m/\u001b[0m\u001b[35m/\u001b[0m \u001b[32m'risk present'\u001b[0m : this means there is evidence this risk is present in the case \u001b[32m'risk not present'\u001b[0m :\n",
       "there is evidence the risk is NOT present or there is no evidence whether the case contains that risk or not. \u001b[1m(\u001b[0mIf \n",
       "in doubt, it is better to err on the side of caution and say \u001b[32m'risk present'\u001b[0m\u001b[1m)\u001b[0m \u001b[1;36m3\u001b[0m. explanation: \u001b[35m/\u001b[0m\u001b[35m/\u001b[0m State words/terms \n",
       "that indicate the reason the risk_output was chosen. Be brief in your explanation. State facts found in the text, \n",
       "do not infer. E.g. \u001b[32m'Client expressed suicidal ideation'\u001b[0m. Leave blank for \u001b[32m'risk not present.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Output Format:</span> │\n",
       "╰────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────╮\n",
       "│ \u001b[1;32mOutput Format:\u001b[0m │\n",
       "╰────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Output should be STRICT JSON, containing: dictionary containing the type of risk with their output and explanation,\n",
       "formatted like this: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'risk_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'suicidality'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'risk_output'</span>: str, <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: str<span style=\"font-weight: bold\">}</span>'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Output should be STRICT JSON, containing: dictionary containing the type of risk with their output and explanation,\n",
       "formatted like this: \u001b[1m{\u001b[0m\u001b[32m'risk_type'\u001b[0m: \u001b[32m'suicidality'\u001b[0m, \u001b[32m'risk_output'\u001b[0m: str, \u001b[32m'explanation'\u001b[0m: str\u001b[1m}\u001b[0m'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                         Comparison of All Iterations                         </span>\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Iteration </span>┃<span style=\"font-weight: bold\"> Precision </span>┃<span style=\"font-weight: bold\"> Recall </span>┃<span style=\"font-weight: bold\"> Accuracy </span>┃<span style=\"font-weight: bold\"> F1-score </span>┃<span style=\"font-weight: bold\"> Invalid Predictions </span>┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│     1     │    <span style=\"font-weight: bold\">0.0000</span> │ <span style=\"font-weight: bold\">0.0000</span> │   <span style=\"font-weight: bold\">0.5000</span> │   <span style=\"font-weight: bold\">0.0000</span> │              <span style=\"font-weight: bold\">2.0000</span> │\n",
       "│     2     │    <span style=\"font-weight: bold\">0.0000</span> │ <span style=\"font-weight: bold\">0.0000</span> │   0.0000 │   <span style=\"font-weight: bold\">0.0000</span> │             10.0000 │\n",
       "│     3     │    <span style=\"font-weight: bold\">0.0000</span> │ <span style=\"font-weight: bold\">0.0000</span> │   0.0000 │   <span style=\"font-weight: bold\">0.0000</span> │             10.0000 │\n",
       "└───────────┴───────────┴────────┴──────────┴──────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                         Comparison of All Iterations                         \u001b[0m\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mIteration\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPrecision\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRecall\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAccuracy\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mF1-score\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInvalid Predictions\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│     1     │    \u001b[1m0.0000\u001b[0m │ \u001b[1m0.0000\u001b[0m │   \u001b[1m0.5000\u001b[0m │   \u001b[1m0.0000\u001b[0m │              \u001b[1m2.0000\u001b[0m │\n",
       "│     2     │    \u001b[1m0.0000\u001b[0m │ \u001b[1m0.0000\u001b[0m │   0.0000 │   \u001b[1m0.0000\u001b[0m │             10.0000 │\n",
       "│     3     │    \u001b[1m0.0000\u001b[0m │ \u001b[1m0.0000\u001b[0m │   0.0000 │   \u001b[1m0.0000\u001b[0m │             10.0000 │\n",
       "└───────────┴───────────┴────────┴──────────┴──────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All logs saved in directory: /Users/danielfiuzadosil/Documents/GitHub/Data-Science/LLMs/iterative_prompt_optimisation/runs/prompt_optimization_logs_20240925_152258\n",
      "\n",
      "Best Prompt:\n",
      "Please look for the following risk factor: Is the client at risk of self-harm? For instance, do they mention suicidal thoughts or ideation? Do they imply they might do physical damage to themselves or to property? Do they reference wanting to 'end it all or say it's 'not worth living'? Please output: 1. risk_type: // suicidality, 2. risk_output: // 'risk present' : this means there is evidence this risk is present in the case 'risk not present' : there is evidence the risk is NOT present or there is no evidence whether the case contains that risk or not. (If in doubt, it is better to err on the side of caution and say 'risk present') 3. explanation: // State words/terms that indicate the reason the risk_output was chosen. Be brief in your explanation. State facts found in the text, do not infer. E.g. 'Client expressed suicidal ideation'. Leave blank for 'risk not present.'\n",
      "\n",
      "Best Metrics:\n",
      "{'precision': 0.0, 'recall': 0.0, 'accuracy': 0.5, 'f1': 0.0, 'predictions': [0, 0, 0, 0, 0, 0, 0, 0], 'false_positives': [], 'false_negatives': [{'text': 'When I was a kid, I loved \"Tiny Toons\". I especially loved \"Tiny Toons: How I spent my Summer Vacation\". I thought it was laughs on the floor funny. A few years later, my friend had the video. And I figured I\\'d watch it for the good old days. I was still on the floors laughing. My opinion, the Plucky and Hampton skit is the best. They decide to go to \"Happy World Land\". And they end up having a crazy adventure to get there. All the skits are funny. I\\'m still looking for the video. So, if anyone has any tips. Please write me.  This is one of the funniest cartoons I have ever seen. 10/10', 'label': 1}, {'text': \"I do not have much to say than this is a great finish to the story. Most people have said that there is not enough plot and its just eye candy.But think about it, everything was explained in FFVII you cannot add more plot to such a grand story it would ruin it. They did the best that they could do and I think that this should be taken more as A Final FMV.. the last fight.  Graphics - 10/10, Absolutely amazing  Story 8.5/10 - don't think they could of expanded it that much more. And the stuff they could put in there was clever enough I thought.  Characters 9/10 - Well most of them have already been explained during the game but still could not fit it all into 90 mins.  Sound - 10/10, since i am a metal fan I loved the fight music.. and piano just reaches right in there..It is a great ST and I was not disappointed. Tilt/Replay 10/10 - Enjoyable every time.  Overall- 9/10 (FF Fan View 10/10) I personally think this is what was needed, a fight to end it all.. the plot was already in place. The action was necessary as much as people complained. I loved every second of this movie. It was a pleasure to visit the world of FFVII just one last time. Just remember this.. most movies that have been made form a game have been directed by movie directors i think this is pretty great for a team of game directors.. Don't think I've seen a better game to movie.. Thankyou Square, I think you did it right!\", 'label': 1}, {'text': 'Caution: May contain spoilers... I\\'ve seen this movie 3 times & I\\'ve liked it every time. Upon seeing it again, I\\'m always reminded of how good it is. An HBO TV movie- very well done like most of their movies are- this would\\'ve gotten Oscars for it\\'s performances had it been released for general distribution instead of made for TV. As I\\'m sure anyone knows from reading other reviews here, this is the story of serial murderer, Andrei Chikatilo. He murdered 56 people over 8 years in the former Soviet Union. (3 victims were buried & couldn\\'t be found so he was only convicted of 52 out of 53 of his murders.) The story actually focuses more on the forensic analyst, Victor Burakov played to perfection by Stephen Rea. A man that becomes tortured and obsessed with finding this killer despite the additional obstacles placed by party hacks, his part is essential to be sure. There is a very touching scene towards the end of the movie that mentions how in America, investigators are routinely taken off serial killer cases after 18 months whether they want to or not due to the mental strain & frustration. According to this acct, Burakov worked for over 5 years before getting his first break from it. He followed the case to its conclusion, 3 years later. In this scene, his superior, General Fetisov, played by Donald Sutherland, actually tells him he admires his dedication and apologizes for not knowing he should\\'ve given him a break sooner. Rea\\'s performance is so well done, he doesn\\'t overact, chew up the scenery or do anything that distracts from his portrayal of a man who is hell bent on finding his killer. He is a man with passion, but doesn\\'t show it in the same manner as is so usually portrayed in detective movies. He only occasionally gives outbursts after quietly putting up with more than most could stand under such circumstances. Rea does so much with his face, his eyes, he doesn\\'t need to overact. He just *is* - His character, so frustrated after so long, at one point, driven to frustration, he actually says he\\'d rather find 3 at one time than none in a year. Of course what he means is not that he wants more people to die, he just wants some clues to catch this man. Rea makes us feel for this man. He makes us understand but a glimpse of what it is to live with such horror and futility. A mutant to be sure, Chikatilo\\'s childhood was one which produces such \"monsters.\" The character of Chikatilo is very well done by Jeffrey DeMunn. He somehow (impossible though it may seem) elicits some modicum of sympathy for himself. Perhaps he is the worst of us gone terribly wrong? Either way, his performance is very well done. Donald Sutherland as Colonel Fetisov (later promoted to General) also does a great job. He starts out seeming to be a cynical worldly official that doesn\\'t seem much more interested in helping the investigation than anyone else blocking Burakov. But he eventually becomes more than just an assistant, he actually actively participates in helping Burakov. There is also a very nice turn by Max Von Sydow as the psychiatrist brought in to help profile and figure out what kind of deviant they are looking for. Although this movie deals with a morbid, grotesque and violent story, it really is more about what it takes to catch a killer than the killer himself. All around a very well done movie with fine performances and a great screenplay. The screenplay manages to do what the best of this type of movie does: give factual events & place them meaningfully inside a dramatic framework that makes you feel like you know the people *behind* the facts. 9 out of 10 stars', 'label': 1}, {'text': \"The idea ia a very short film with a lot of information. Interesting, entertaining and leaves the viewer wanting more. The producer has produced a short film of excellent quality that cannot be compared to any other short film that I have seen. I have rated this film at the highest possible rating. I also recommend that it is shown to office managers and business people in any establishment. What comes out of it is the fact that people with ideas are never listened to, their voice is never heard. It is a lesson to be learned by any office that wants to go forward. I hope that the produced will produce a second part to this 'idea'. I look forward to viewing the sequence. Once again congrats to Halaqah media in producing a film of excellence and quality with a lesson in mind.\", 'label': 1}], 'invalid_predictions': 2}\n"
     ]
    }
   ],
   "source": [
    "# Run the prompt optimization process\n",
    "best_prompt, best_metrics = optimize_prompt(\n",
    "    initial_prompt,\n",
    "    output_format_prompt,\n",
    "    eval_data,\n",
    "    iterations,\n",
    "    eval_provider=eval_provider,\n",
    "    eval_model=eval_model,\n",
    "    optim_provider=optim_provider,\n",
    "    optim_model=optim_model,\n",
    "    output_schema=output_schema\n",
    ")\n",
    "print(\"\\nBest Prompt:\")\n",
    "print(best_prompt)\n",
    "print(\"\\nBest Metrics:\")\n",
    "print(best_metrics)\n",
    "\n",
    "# After running the optimization process, you can analyze the results by checking \n",
    "# the generated log files in the `runs/prompt_optimization_logs_YYYYMMDD_HHMMSS` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
