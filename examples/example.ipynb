{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define initial prompt and output format\n",
    "# initial_prompt = (\n",
    "#     \"Please look for the following risk factor: Is the client at risk of self-harm? For instance, do they mention suicidal thoughts or ideation? Do they imply they might do physical damage to themselves or to property? Do they reference wanting to 'end it all or say it's 'not worth living'? Please output: 1. risk_type: // suicidality, 2. risk_output: // 'risk present' : this means there is evidence this risk is present in the case 'risk not present' : there is evidence the risk is NOT present or there is no evidence whether the case contains that risk or not. (If in doubt, it is better to err on the side of caution and say 'risk present') 3. explanation: // State words/terms that indicate the reason the risk_output was chosen. Be brief in your explanation. State facts found in the text, do not infer. E.g. 'Client expressed suicidal ideation'. Leave blank for 'risk not present.'\"\n",
    "# )\n",
    "\n",
    "# output_format_prompt = (\n",
    "#     \"Output should be STRICT JSON, containing: dictionary containing the type of risk with their output and explanation, formatted like this: {'risk_type': 'suicidality', 'risk_output': str, 'explanation': str}'\"\n",
    "# )\n",
    "\n",
    "initial_prompt = (\n",
    "        \"You are a sentiment analysis classifier. Determine whether the provided text expresses a positive sentiment. \"\n",
    "        \"Respond with '1' if it is positive, or '0' if it is negative.\"\n",
    "    )\n",
    "\n",
    "# Output format prompt\n",
    "output_format_prompt = (\n",
    "    \"You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for negative. \"\n",
    "    \"Do NOT include any additional text or explanation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define output schema\n",
    "# output_schema = {\n",
    "#     'key_to_extract': 'risk_output',\n",
    "#     'value_mapping': {\n",
    "#         'risk_present': 1,\n",
    "#         'risk_not_present': 0\n",
    "#     },\n",
    "#     'regex_pattern': r'\"risk_output\":\\s*\"(.*?)\"'\n",
    "# }\n",
    "\n",
    "output_schema = {\n",
    "    'key_to_extract': None,  # Set to None for direct output\n",
    "    'value_mapping': None,   # Set to None for direct mapping\n",
    "    'regex_pattern': r'^(0|1)$'  # Match the entire output for binary classification\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of optimization iterations\n",
    "iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model providers and models for evaluation and optimization\n",
    "eval_provider = \"ollama\"\n",
    "eval_model = \"llama3.1\"\n",
    "optim_provider = \"ollama\"\n",
    "optim_model = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing review data for evaluation\n",
    "eval_datapath = \"reviews.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to sys.path\n",
    "# Use getcwd() to get the current working directory for Jupyter notebooks\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from src.iterative_prompt_optimization import optimize_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation data shape: (10, 2)\n",
      "                                                text  label\n",
      "0  Was this based on a comic-book? A video-game? ...      1\n",
      "1  If you ask me the first one was really better ...      0\n",
      "2  When I was a kid, I loved \"Tiny Toons\". I espe...      1\n",
      "3  I hate guns and have never murdered anyone, bu...      0\n",
      "4  I do not have much to say than this is a great...      1\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "eval_data = pd.read_csv(eval_datapath, encoding='ISO-8859-1', usecols=['Text', 'Sentiment'])\n",
    "eval_data.columns = ['text', 'label']\n",
    "# Randomly select 50 positive and 50 negative samples\n",
    "eval_data = (\n",
    "    eval_data.groupby('label')\n",
    "    .apply(lambda x: x.sample(n=5, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "# Shuffle the DataFrame randomly\n",
    "eval_data = eval_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Evaluation data shape: {eval_data.shape}\")\n",
    "print(eval_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected evaluation provider: ollama\n",
      "Selected evaluation model: llama3.1\n",
      "Selected optimization provider: ollama\n",
      "Selected optimization model: llama3.1\n",
      "Estimated token usage: 107190\n",
      "Estimated cost: $0 API Costs - Running on Local Hardware\n",
      "\n",
      "Do you want to proceed with the optimization? (Y/N): \n",
      "Iteration 1/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭────────────────────────────────────────────── Current Full Prompt ──────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> You are a sentiment analysis classifier. Determine whether the provided text expresses a positive sentiment.    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Respond with '1' if it is positive, or '0' if it is negative.                                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> negative. Do NOT include any additional text or explanation.                                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m Current Full Prompt \u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m You are a sentiment analysis classifier. Determine whether the provided text expresses a positive sentiment.    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m Respond with '1' if it is positive, or '0' if it is negative.                                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m negative. Do NOT include any additional text or explanation.                                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 1/10\n",
      "Prediction 1/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 2/10\n",
      "Prediction 2/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 3/10\n",
      "Prediction 3/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 4/10\n",
      "Prediction 4/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 5/10\n",
      "Prediction 5/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 6/10\n",
      "Prediction 6/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 7/10\n",
      "Prediction 7/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 8/10\n",
      "Prediction 8/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 9/10\n",
      "Prediction 9/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 10/10\n",
      "Prediction 10/10: 1 | Ground Truth: 1 ✅ (TP)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 1</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.7143 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8333 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 1\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.7143 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8333 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassifications...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">╭──────────────────────────────────────── Analysis of Misclassifications ─────────────────────────────────────────╮</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> After analyzing the misclassifications, I've identified several patterns that contributed to the model's        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> mistakes. Here are my findings:                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **False Positives (Negative texts incorrectly classified as Positive):**                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Overemphasis on violent action**: The first example praises the movie \"Shuttle\" for its realistic and      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> intense violence, which might have led the model to misclassify it as a positive review.                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Example: \"...YOU SHOOT YOUR ATTACKER. THREE TIMES. FIVE TIMES... I think even God would say, 'Good    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> call.'\"                                                                                                         <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Correct classification: Negative (the reviewer criticizes the movie's clichés and predictability)     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Use of hyperbole**: The second example uses exaggerated language to describe the movie \"Dragon Hunt\" as    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"great trash,\" which might have misled the model into thinking it was a positive review.                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Example: \"...if you are into great trash, 'Dragon Hunt' is made for you.\"                             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Correct classification: Positive (the reviewer explicitly states that the movie is intended for fans  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> of bad cinema)                                                                                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3. **Tone and irony**: Both examples use an ironic or sarcastic tone when discussing the movies' shortcomings,  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> which might have been misinterpreted by the model.                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Example 1: \"...it would've served the audience better with roughly 15-20 minutes deleted...\"          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Correct classification: Negative (the reviewer criticizes the movie's length and predictability)      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Example 2: \"...this movie is really awful, but then again, it is a great party tape!\"                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Correct classification: Positive (the reviewer acknowledges the movie's poor quality while praising   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> its entertainment value)                                                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **False Negatives (Positive texts incorrectly classified as Negative):**                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Since there were no examples of positive texts incorrectly classified as negative in the provided data, I'll    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> focus on strategies to improve the classification prompt for reducing false positives and false negatives.      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Recommendations:**                                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Improve handling of violent content**: The model should be more nuanced when dealing with violent or       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> intense action scenes. It might need to consider the context and tone of the review when deciding whether a     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> text is positive or negative.                                                                                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Enhance understanding of irony and sarcasm**: The prompt should be updated to better detect ironic or      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> sarcastic language, which can be misleading in text classification tasks.                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3. **Introduce nuance to tone analysis**: The model should consider the reviewer's tone more accurately, as     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> it's not always possible to determine whether a review is positive or negative based solely on the words used.  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 4. **Incorporate context-aware processing**: The prompt could benefit from incorporating contextual             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> information, such as the movie's genre, release date, and audience expectations, to better understand the       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> reviewer's intent and tone.                                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 5. **Use more advanced Natural Language Processing (NLP) techniques**: Consider using techniques like           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> aspect-based sentiment analysis or multimodal fusion to improve the model's understanding of text and its       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> classification.                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> By implementing these strategies, the classification prompt can become more sensitive to subtle differences in  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> language and better recognize the nuances that led to the errors. This will help reduce both false positives    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> and false negatives, leading to more accurate text classifications.                                             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m╭─\u001b[0m\u001b[1m───────────────────────────────────────\u001b[0m\u001b[1m Analysis of Misclassifications \u001b[0m\u001b[1m────────────────────────────────────────\u001b[0m\u001b[1m─╮\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m After analyzing the misclassifications, I've identified several patterns that contributed to the model's        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m mistakes. Here are my findings:                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **False Positives (Negative texts incorrectly classified as Positive):**                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Overemphasis on violent action**: The first example praises the movie \"Shuttle\" for its realistic and      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m intense violence, which might have led the model to misclassify it as a positive review.                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Example: \"...YOU SHOOT YOUR ATTACKER. THREE TIMES. FIVE TIMES... I think even God would say, 'Good    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m call.'\"                                                                                                         \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Correct classification: Negative (the reviewer criticizes the movie's clichés and predictability)     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Use of hyperbole**: The second example uses exaggerated language to describe the movie \"Dragon Hunt\" as    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"great trash,\" which might have misled the model into thinking it was a positive review.                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Example: \"...if you are into great trash, 'Dragon Hunt' is made for you.\"                             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Correct classification: Positive (the reviewer explicitly states that the movie is intended for fans  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m of bad cinema)                                                                                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3. **Tone and irony**: Both examples use an ironic or sarcastic tone when discussing the movies' shortcomings,  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m which might have been misinterpreted by the model.                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Example 1: \"...it would've served the audience better with roughly 15-20 minutes deleted...\"          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Correct classification: Negative (the reviewer criticizes the movie's length and predictability)      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Example 2: \"...this movie is really awful, but then again, it is a great party tape!\"                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Correct classification: Positive (the reviewer acknowledges the movie's poor quality while praising   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m its entertainment value)                                                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **False Negatives (Positive texts incorrectly classified as Negative):**                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Since there were no examples of positive texts incorrectly classified as negative in the provided data, I'll    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m focus on strategies to improve the classification prompt for reducing false positives and false negatives.      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Recommendations:**                                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Improve handling of violent content**: The model should be more nuanced when dealing with violent or       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m intense action scenes. It might need to consider the context and tone of the review when deciding whether a     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m text is positive or negative.                                                                                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Enhance understanding of irony and sarcasm**: The prompt should be updated to better detect ironic or      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m sarcastic language, which can be misleading in text classification tasks.                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3. **Introduce nuance to tone analysis**: The model should consider the reviewer's tone more accurately, as     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m it's not always possible to determine whether a review is positive or negative based solely on the words used.  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 4. **Incorporate context-aware processing**: The prompt could benefit from incorporating contextual             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m information, such as the movie's genre, release date, and audience expectations, to better understand the       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m reviewer's intent and tone.                                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 5. **Use more advanced Natural Language Processing (NLP) techniques**: Consider using techniques like           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m aspect-based sentiment analysis or multimodal fusion to improve the model's understanding of text and its       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m classification.                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m By implementing these strategies, the classification prompt can become more sensitive to subtle differences in  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m language and better recognize the nuances that led to the errors. This will help reduce both false positives    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m and false negatives, leading to more accurate text classifications.                                             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new prompt...\n",
      "\n",
      "Iteration 2/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭────────────────────────────────────────────── Current Full Prompt ──────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Here's a refined version of the prompt:                                                                         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> I'm a nuanced sentiment analysis classifier. When analyzing text about movies, I must consider various          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> contextual factors and language nuances to accurately determine whether the text expresses a positive           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> sentiment.                                                                                                      <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> To make this determination, I'll take into account:                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> * The tone and intent behind the reviewer's words, including potential irony, sarcasm, or hyperbole.            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> * The context of the review, such as the movie's genre, release date, and audience expectations.                <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> * The presence and intensity of violent or intense action scenes, considering whether they're discussed in a    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> positive, negative, or neutral light.                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> I'll respond with '1' if I'm convinced that the text expresses a positive sentiment, and '0' otherwise.         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> negative. Do NOT include any additional text or explanation.                                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m Current Full Prompt \u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m Here's a refined version of the prompt:                                                                         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m I'm a nuanced sentiment analysis classifier. When analyzing text about movies, I must consider various          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m contextual factors and language nuances to accurately determine whether the text expresses a positive           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m sentiment.                                                                                                      \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m To make this determination, I'll take into account:                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m * The tone and intent behind the reviewer's words, including potential irony, sarcasm, or hyperbole.            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m * The context of the review, such as the movie's genre, release date, and audience expectations.                \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m * The presence and intensity of violent or intense action scenes, considering whether they're discussed in a    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m positive, negative, or neutral light.                                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m I'll respond with '1' if I'm convinced that the text expresses a positive sentiment, and '0' otherwise.         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m negative. Do NOT include any additional text or explanation.                                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 1/10\n",
      "Prediction 1/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 2/10\n",
      "Prediction 2/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 3/10\n",
      "Prediction 3/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 4/10\n",
      "Prediction 4/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 5/10\n",
      "Prediction 5/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 6/10\n",
      "Prediction 6/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 7/10\n",
      "Prediction 7/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 8/10\n",
      "Prediction 8/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 9/10\n",
      "Prediction 9/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 10/10\n",
      "Prediction 10/10: 1 | Ground Truth: 1 ✅ (TP)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 2</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.7143 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8333 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 2\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.7143 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8333 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassifications...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">╭──────────────────────────────────────── Analysis of Misclassifications ─────────────────────────────────────────╮</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Analysis of Misclassifications**                                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> The provided texts are examples of misclassified binary classifications, where the LLM model incorrectly        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> classified a \"Negative\" (0) text as Positive and vice versa. Upon analyzing these texts, I've identified        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> specific examples where the model made mistakes.                                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Misclassified Negative (0) Texts**                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Incorrect classification as Positive:**                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>    - Example: \"...YOU SHOOT YOUR ATTACKER. THREE TIMES. FIVE TIMES... Good call.\"                               <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>      * This sentence is written in a tone that suggests the speaker's strong anger and willingness to take      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> drastic measures, but it doesn't necessarily indicate they enjoyed or appreciated the movie.                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>        Correct classification: Negative (0)                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>      * The correct classification should have been Negative because, despite the speaker's justification for    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> violence, their tone still conveys negativity towards violence and the situation.                               <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Incorrect classification as Positive:**                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>    - Example: \"...'Shuttle' was a decent film... getting past those annoyances...\"                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>        Correct classification: Negative (0)                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>      * The speaker mentions that 'Shuttle' is \"decent\" but lists several negative aspects of the movie, such as <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> predictable plot and annoying moments. Despite this mixed review, the text leans towards being Negative due to  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> its emphasis on flaws.                                                                                          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Misclassified Positive (1) Texts**                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> There are no provided examples for Positive texts incorrectly classified as Negative.                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> However, if you'd like me to analyze hypothetical examples of Positive texts misclassified as Negative or vice  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> versa based on typical characteristics and provide strategies to improve the classification prompt, I can       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> certainly do that.                                                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Strategies to Improve Classification Prompt**                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1.  **Expand Contextual Understanding**: To reduce false positives and negatives, consider adding               <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> context-specific features such as:                                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>     *   Emotion analysis: Integrate sentiment analysis to better capture nuances in emotional tone and infer    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> whether a text leans towards being positive or negative.                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>     *   Topic modeling: Incorporate topic models to understand the underlying themes in the text. For instance, <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> if a text discusses the positives and negatives of an experience but concludes with a strong positive note,     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> this could indicate that the overall sentiment is more positive than initially apparent.                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2.  **Enhance Language Understanding**: Improve language understanding by leveraging techniques such as:        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>     *   Aspect-based sentiment analysis: Analyze the impact on different aspects (e.g., plot, characters, and   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> direction) in reviews or comments to better understand the nuances of opinions.                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>     *   Integrate knowledge about common tropes and clichés: Consider incorporating domain-specific knowledge   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> to recognize patterns that might influence classification. For example, a \"Hostel\" knock-off has certain        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> expectations that could impact the overall sentiment.                                                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3.  **Enrich Training Data**: Ensure your training data accurately represents various writing styles, emotions, <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> and nuances. Incorporate diverse perspectives through techniques such as:                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>     *   Active learning: Interactively collect more relevant examples of misclassified texts to train models.   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>     *   Transfer learning: Leverage pre-trained models fine-tuned on specific domains or genres that might      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> better capture subtle differences in text classification.                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 4.  **Regular Model Refresh and Evaluation**: Periodically update your model with new data, retrain it using    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> enhanced techniques, and evaluate its performance to address emerging biases or errors.                         <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 5.  **Incorporate Human Feedback Mechanisms**: Allow users to provide feedback on incorrect classifications.    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> This can help you identify patterns of misclassification and improve your models accordingly.                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> By incorporating these strategies into the classification prompt and continually refining them with the latest  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> advances in NLP, you should see a reduction in both false positives and negatives while improving overall model <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> performance.                                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m╭─\u001b[0m\u001b[1m───────────────────────────────────────\u001b[0m\u001b[1m Analysis of Misclassifications \u001b[0m\u001b[1m────────────────────────────────────────\u001b[0m\u001b[1m─╮\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Analysis of Misclassifications**                                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m The provided texts are examples of misclassified binary classifications, where the LLM model incorrectly        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m classified a \"Negative\" (0) text as Positive and vice versa. Upon analyzing these texts, I've identified        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m specific examples where the model made mistakes.                                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Misclassified Negative (0) Texts**                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Incorrect classification as Positive:**                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m    - Example: \"...YOU SHOOT YOUR ATTACKER. THREE TIMES. FIVE TIMES... Good call.\"                               \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m      * This sentence is written in a tone that suggests the speaker's strong anger and willingness to take      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m drastic measures, but it doesn't necessarily indicate they enjoyed or appreciated the movie.                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m        Correct classification: Negative (0)                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m      * The correct classification should have been Negative because, despite the speaker's justification for    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m violence, their tone still conveys negativity towards violence and the situation.                               \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Incorrect classification as Positive:**                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m    - Example: \"...'Shuttle' was a decent film... getting past those annoyances...\"                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m        Correct classification: Negative (0)                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m      * The speaker mentions that 'Shuttle' is \"decent\" but lists several negative aspects of the movie, such as \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m predictable plot and annoying moments. Despite this mixed review, the text leans towards being Negative due to  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m its emphasis on flaws.                                                                                          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Misclassified Positive (1) Texts**                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m There are no provided examples for Positive texts incorrectly classified as Negative.                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m However, if you'd like me to analyze hypothetical examples of Positive texts misclassified as Negative or vice  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m versa based on typical characteristics and provide strategies to improve the classification prompt, I can       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m certainly do that.                                                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Strategies to Improve Classification Prompt**                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1.  **Expand Contextual Understanding**: To reduce false positives and negatives, consider adding               \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m context-specific features such as:                                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m     *   Emotion analysis: Integrate sentiment analysis to better capture nuances in emotional tone and infer    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m whether a text leans towards being positive or negative.                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m     *   Topic modeling: Incorporate topic models to understand the underlying themes in the text. For instance, \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m if a text discusses the positives and negatives of an experience but concludes with a strong positive note,     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m this could indicate that the overall sentiment is more positive than initially apparent.                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2.  **Enhance Language Understanding**: Improve language understanding by leveraging techniques such as:        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m     *   Aspect-based sentiment analysis: Analyze the impact on different aspects (e.g., plot, characters, and   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m direction) in reviews or comments to better understand the nuances of opinions.                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m     *   Integrate knowledge about common tropes and clichés: Consider incorporating domain-specific knowledge   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m to recognize patterns that might influence classification. For example, a \"Hostel\" knock-off has certain        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m expectations that could impact the overall sentiment.                                                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3.  **Enrich Training Data**: Ensure your training data accurately represents various writing styles, emotions, \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m and nuances. Incorporate diverse perspectives through techniques such as:                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m     *   Active learning: Interactively collect more relevant examples of misclassified texts to train models.   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m     *   Transfer learning: Leverage pre-trained models fine-tuned on specific domains or genres that might      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m better capture subtle differences in text classification.                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 4.  **Regular Model Refresh and Evaluation**: Periodically update your model with new data, retrain it using    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m enhanced techniques, and evaluate its performance to address emerging biases or errors.                         \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 5.  **Incorporate Human Feedback Mechanisms**: Allow users to provide feedback on incorrect classifications.    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m This can help you identify patterns of misclassification and improve your models accordingly.                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m By incorporating these strategies into the classification prompt and continually refining them with the latest  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m advances in NLP, you should see a reduction in both false positives and negatives while improving overall model \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m performance.                                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new prompt...\n",
      "\n",
      "Iteration 3/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭────────────────────────────────────────────── Current Full Prompt ──────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> I'm a nuanced sentiment analysis classifier. When analyzing text about movies, I must consider various          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> contextual factors and language nuances to accurately determine whether the text expresses a positive           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> sentiment.                                                                                                      <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> To make this determination, I'll take into account:                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> *   The tone and intent behind the reviewer's words, including potential irony, sarcasm, or hyperbole.          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> *   Contextual information such as the movie's genre, release date, audience expectations, presence and         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> intensity of violent or intense action scenes, considering whether they're discussed in a positive, negative,   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> or neutral light.                                                                                               <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> *   The impact on different aspects (e.g., plot, characters, direction) to understand nuances of opinions.      <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> I'll also consider emotion analysis, topic modeling, aspect-based sentiment analysis, common tropes and         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> clichés, diverse perspectives, and human feedback mechanisms when making my determination.                      <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> I'll respond with '1' if I'm convinced that the text expresses a positive sentiment, and '0' otherwise.         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> negative. Do NOT include any additional text or explanation.                                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m Current Full Prompt \u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m I'm a nuanced sentiment analysis classifier. When analyzing text about movies, I must consider various          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m contextual factors and language nuances to accurately determine whether the text expresses a positive           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m sentiment.                                                                                                      \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m To make this determination, I'll take into account:                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m *   The tone and intent behind the reviewer's words, including potential irony, sarcasm, or hyperbole.          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m *   Contextual information such as the movie's genre, release date, audience expectations, presence and         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m intensity of violent or intense action scenes, considering whether they're discussed in a positive, negative,   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m or neutral light.                                                                                               \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m *   The impact on different aspects (e.g., plot, characters, direction) to understand nuances of opinions.      \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m I'll also consider emotion analysis, topic modeling, aspect-based sentiment analysis, common tropes and         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m clichés, diverse perspectives, and human feedback mechanisms when making my determination.                      \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m I'll respond with '1' if I'm convinced that the text expresses a positive sentiment, and '0' otherwise.         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m negative. Do NOT include any additional text or explanation.                                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 1/10\n",
      "Prediction 1/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 2/10\n",
      "Prediction 2/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 3/10\n",
      "Prediction 3/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 4/10\n",
      "Prediction 4/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 5/10\n",
      "Prediction 5/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 6/10\n",
      "Prediction 6/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 7/10\n",
      "Prediction 7/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 8/10\n",
      "Prediction 8/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 9/10\n",
      "Prediction 9/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 10/10\n",
      "Prediction 10/10: 1 | Ground Truth: 1 ✅ (TP)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 3</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.7143 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8333 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 3\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.7143 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8333 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Best Prompt:</span> │\n",
       "╰──────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────╮\n",
       "│ \u001b[1;32mBest Prompt:\u001b[0m │\n",
       "╰──────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are a sentiment analysis classifier. Determine whether the provided text expresses a positive sentiment. \n",
       "Respond with <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span> if it is positive, or <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span> if it is negative.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are a sentiment analysis classifier. Determine whether the provided text expresses a positive sentiment. \n",
       "Respond with \u001b[32m'1'\u001b[0m if it is positive, or \u001b[32m'0'\u001b[0m if it is negative.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Output Format:</span> │\n",
       "╰────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────╮\n",
       "│ \u001b[1;32mOutput Format:\u001b[0m │\n",
       "╰────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are to act as a binary responder. For every question asked, reply strictly with <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span> for positive or <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span> for \n",
       "negative. Do NOT include any additional text or explanation.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are to act as a binary responder. For every question asked, reply strictly with \u001b[32m'1'\u001b[0m for positive or \u001b[32m'0'\u001b[0m for \n",
       "negative. Do NOT include any additional text or explanation.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                         Comparison of All Iterations                         </span>\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Iteration </span>┃<span style=\"font-weight: bold\"> Precision </span>┃<span style=\"font-weight: bold\"> Recall </span>┃<span style=\"font-weight: bold\"> Accuracy </span>┃<span style=\"font-weight: bold\"> F1-score </span>┃<span style=\"font-weight: bold\"> Invalid Predictions </span>┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│     1     │    <span style=\"font-weight: bold\">0.7143</span> │ <span style=\"font-weight: bold\">1.0000</span> │   <span style=\"font-weight: bold\">0.8000</span> │   <span style=\"font-weight: bold\">0.8333</span> │              <span style=\"font-weight: bold\">0.0000</span> │\n",
       "│     2     │    <span style=\"font-weight: bold\">0.7143</span> │ <span style=\"font-weight: bold\">1.0000</span> │   <span style=\"font-weight: bold\">0.8000</span> │   <span style=\"font-weight: bold\">0.8333</span> │              <span style=\"font-weight: bold\">0.0000</span> │\n",
       "│     3     │    <span style=\"font-weight: bold\">0.7143</span> │ <span style=\"font-weight: bold\">1.0000</span> │   <span style=\"font-weight: bold\">0.8000</span> │   <span style=\"font-weight: bold\">0.8333</span> │              <span style=\"font-weight: bold\">0.0000</span> │\n",
       "└───────────┴───────────┴────────┴──────────┴──────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                         Comparison of All Iterations                         \u001b[0m\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mIteration\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPrecision\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRecall\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAccuracy\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mF1-score\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInvalid Predictions\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│     1     │    \u001b[1m0.7143\u001b[0m │ \u001b[1m1.0000\u001b[0m │   \u001b[1m0.8000\u001b[0m │   \u001b[1m0.8333\u001b[0m │              \u001b[1m0.0000\u001b[0m │\n",
       "│     2     │    \u001b[1m0.7143\u001b[0m │ \u001b[1m1.0000\u001b[0m │   \u001b[1m0.8000\u001b[0m │   \u001b[1m0.8333\u001b[0m │              \u001b[1m0.0000\u001b[0m │\n",
       "│     3     │    \u001b[1m0.7143\u001b[0m │ \u001b[1m1.0000\u001b[0m │   \u001b[1m0.8000\u001b[0m │   \u001b[1m0.8333\u001b[0m │              \u001b[1m0.0000\u001b[0m │\n",
       "└───────────┴───────────┴────────┴──────────┴──────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All logs saved in directory: /Users/danielfiuzadosil/Documents/GitHub/AI-Prompt-Optimiser/runs/prompt_optimization_logs_20240925_171452\n"
     ]
    }
   ],
   "source": [
    "# Run the prompt optimization process\n",
    "best_prompt, best_metrics = optimize_prompt(\n",
    "    initial_prompt,\n",
    "    output_format_prompt,\n",
    "    eval_data,\n",
    "    iterations,\n",
    "    eval_provider=eval_provider,\n",
    "    eval_model=eval_model,\n",
    "    optim_provider=optim_provider,\n",
    "    optim_model=optim_model,\n",
    "    output_schema=output_schema\n",
    ")\n",
    "# After running the optimization process, you can analyze the results by checking \n",
    "# the generated log files in the `runs/prompt_optimization_logs_YYYYMMDD_HHMMSS` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
