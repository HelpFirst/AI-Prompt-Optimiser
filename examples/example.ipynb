{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define initial prompt and output format\n",
    "# initial_prompt = (\n",
    "#     \"Please look for the following risk factor: Is the client at risk of self-harm? For instance, do they mention suicidal thoughts or ideation? Do they imply they might do physical damage to themselves or to property? Do they reference wanting to 'end it all or say it's 'not worth living'? Please output: 1. risk_type: // suicidality, 2. risk_output: // 'risk present' : this means there is evidence this risk is present in the case 'risk not present' : there is evidence the risk is NOT present or there is no evidence whether the case contains that risk or not. (If in doubt, it is better to err on the side of caution and say 'risk present') 3. explanation: // State words/terms that indicate the reason the risk_output was chosen. Be brief in your explanation. State facts found in the text, do not infer. E.g. 'Client expressed suicidal ideation'. Leave blank for 'risk not present.'\"\n",
    "# )\n",
    "\n",
    "# output_format_prompt = (\n",
    "#     \"Output should be STRICT JSON, containing: dictionary containing the type of risk with their output and explanation, formatted like this: {'risk_type': 'suicidality', 'risk_output': str, 'explanation': str}'\"\n",
    "# )\n",
    "\n",
    "initial_prompt = (\n",
    "        \"You are a sentiment analysis classifier. Determine whether the provided text expresses a positive sentiment. \"\n",
    "        \"Respond with '1' if it is positive, or '0' if it is negative.\"\n",
    "    )\n",
    "\n",
    "# Output format prompt\n",
    "output_format_prompt = (\n",
    "    \"You are to act as a binary responder. For every question asked, reply strictly with '1' for positive or '0' for negative. \"\n",
    "    \"Do NOT include any additional text or explanation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define output schema\n",
    "# output_schema = {\n",
    "#     'key_to_extract': 'risk_output',\n",
    "#     'value_mapping': {\n",
    "#         'risk_present': 1,\n",
    "#         'risk_not_present': 0\n",
    "#     },\n",
    "#     'regex_pattern': r'\"risk_output\":\\s*\"(.*?)\"'\n",
    "# }\n",
    "\n",
    "output_schema = {\n",
    "    'key_to_extract': None,  # Set to None for direct output\n",
    "    'value_mapping': None,   # Set to None for direct mapping\n",
    "    'regex_pattern': r'^(0|1)$'  # Match the entire output for binary classification\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of optimization iterations\n",
    "iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model providers and models for evaluation and optimization\n",
    "eval_provider = \"ollama\"\n",
    "eval_model = \"llama3.1\"\n",
    "optim_provider = \"ollama\"\n",
    "optim_model = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing review data for evaluation\n",
    "eval_datapath = \"reviews.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to sys.path\n",
    "# Use getcwd() to get the current working directory for Jupyter notebooks\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from src.iterative_prompt_optimization import optimize_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation data shape: (10, 2)\n",
      "                                                text  label\n",
      "0  Was this based on a comic-book? A video-game? ...      1\n",
      "1  If you ask me the first one was really better ...      0\n",
      "2  When I was a kid, I loved \"Tiny Toons\". I espe...      1\n",
      "3  I hate guns and have never murdered anyone, bu...      0\n",
      "4  I do not have much to say than this is a great...      1\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "eval_data = pd.read_csv(eval_datapath, encoding='ISO-8859-1', usecols=['Text', 'Sentiment'])\n",
    "eval_data.columns = ['text', 'label']\n",
    "# Randomly select 50 positive and 50 negative samples\n",
    "eval_data = (\n",
    "    eval_data.groupby('label')\n",
    "    .apply(lambda x: x.sample(n=5, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "# Shuffle the DataFrame randomly\n",
    "eval_data = eval_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Evaluation data shape: {eval_data.shape}\")\n",
    "print(eval_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected evaluation provider: ollama\n",
      "Selected evaluation model: llama3.1\n",
      "Selected optimization provider: ollama\n",
      "Selected optimization model: llama3.1\n",
      "Estimated token usage: 107190\n",
      "Estimated cost: $0 API Costs - Running on Local Hardware\n",
      "\n",
      "Do you want to proceed with the optimization? (Y/N): \n",
      "Iteration 1/3\n",
      "Processing text 1/10\n",
      "Prediction 1/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 2/10\n",
      "Prediction 2/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 3/10\n",
      "Prediction 3/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 4/10\n",
      "Prediction 4/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 5/10\n",
      "Prediction 5/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 6/10\n",
      "Prediction 6/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 7/10\n",
      "Prediction 7/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 8/10\n",
      "Prediction 8/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 9/10\n",
      "Prediction 9/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 10/10\n",
      "Prediction 10/10: 1 | Ground Truth: 1 ✅ (TP)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 1</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.7143 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8333 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 1\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.7143 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8333 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassifications...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">╭──────────────────────────────────────── Analysis of Misclassifications ─────────────────────────────────────────╮</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Analysis of Misclassifications**                                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Negative (0) texts incorrectly classified as positive:**                                                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * The first example is a mixed review of the movie \"Shuttle\". The reviewer expresses both negative and  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> positive sentiments, but the LLM model incorrectly classified it as positive.                                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                 + Specific mistakes:                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                         - The text states that the reviewer thinks God would say \"Good call\" in a hypothetical  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> situation, which implies a strong emotional response. However, this is not indicative of a positive review      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> overall.                                                                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                         - The reviewer criticizes the movie's clichéd plot and lack of originality, which       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> should be seen as negative traits.                                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                 + Correct classification: Negative (0)                                                          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * The second example is a scathing review of the movie \"Dragon Hunt\". Despite describing it as one of   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> the cheapest action flicks of the eighties, the reviewer still manages to find some positive aspects in it      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> being a great party tape.                                                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                 + Specific mistakes:                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                         - The text highlights several negative elements of the movie, such as its poor          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> production values and ridiculous plot.                                                                          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                         - However, the reviewer's tone is more playful than genuinely positive, which should    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> have been picked up by the LLM model.                                                                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Positive (1) texts incorrectly classified as negative:**                                                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>         * Unfortunately, there are no examples provided for this category.                                      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Insights into why these errors occurred**                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Based on the analysis above, it appears that the LLM model made mistakes in the following areas:                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Difficulty in detecting nuanced sentiment**: The model struggled to distinguish between mixed reviews and  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> genuinely positive ones.                                                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Sensitivity to strong language**: The use of words like \"hate\" and \"ridiculous\" led the model to           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> incorrectly classify texts as negative, even when they contained some positive sentiments.                      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3. **Inability to recognize sarcasm and irony**: The LLM model failed to pick up on the playful tone in the     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> second example, leading it to misclassify the text as negative.                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Strategies to improve the classification prompt**                                                             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> To reduce both false positives and false negatives, I recommend the following strategies:                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Improve sentiment analysis for nuanced reviews**: Modify the prompt to account for mixed reviews with both <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> positive and negative sentiments.                                                                               <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Sensitivity adjustments**: Fine-tune the model's sensitivity to strong language by considering context and <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> tone.                                                                                                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3. **Incorporate sarcasm and irony detection**: Implement techniques to identify when text contains sarcastic   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> or ironic statements, which can lead to incorrect classifications.                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 4. **Training data curation**: Update the training dataset with more diverse examples that showcase nuanced     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> sentiment, sarcasm, and irony.                                                                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 5. **Prompt modification**: Modify the classification prompt to explicitly ask for both positive and negative   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> sentiments, rather than relying solely on a single label.                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Recommendations**                                                                                             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Based on these strategies, I suggest modifying the classification prompt as follows:                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. **Classification Prompt**: Instead of asking \"Is this text positive (1) or negative (0)?\", modify it to:     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> \"What sentiment does this text convey? (Positive, Negative, Mixed)\"                                             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. **Incorporate sentiment analysis for nuanced reviews**: Update the training data and model architecture to   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> better account for mixed reviews with both positive and negative sentiments.                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3. **Implement sarcasm and irony detection**: Integrate techniques to identify when text contains sarcastic or  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> ironic statements, which can lead to incorrect classifications.                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> By implementing these strategies and modifications, we can improve the LLM model's ability to accurately        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> classify binary sentiment texts, reducing false positives and false negatives.                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m╭─\u001b[0m\u001b[1m───────────────────────────────────────\u001b[0m\u001b[1m Analysis of Misclassifications \u001b[0m\u001b[1m────────────────────────────────────────\u001b[0m\u001b[1m─╮\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Analysis of Misclassifications**                                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Negative (0) texts incorrectly classified as positive:**                                                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * The first example is a mixed review of the movie \"Shuttle\". The reviewer expresses both negative and  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m positive sentiments, but the LLM model incorrectly classified it as positive.                                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                 + Specific mistakes:                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                         - The text states that the reviewer thinks God would say \"Good call\" in a hypothetical  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m situation, which implies a strong emotional response. However, this is not indicative of a positive review      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m overall.                                                                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                         - The reviewer criticizes the movie's clichéd plot and lack of originality, which       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m should be seen as negative traits.                                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                 + Correct classification: Negative (0)                                                          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * The second example is a scathing review of the movie \"Dragon Hunt\". Despite describing it as one of   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m the cheapest action flicks of the eighties, the reviewer still manages to find some positive aspects in it      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m being a great party tape.                                                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                 + Specific mistakes:                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                         - The text highlights several negative elements of the movie, such as its poor          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m production values and ridiculous plot.                                                                          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                         - However, the reviewer's tone is more playful than genuinely positive, which should    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m have been picked up by the LLM model.                                                                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Positive (1) texts incorrectly classified as negative:**                                                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m         * Unfortunately, there are no examples provided for this category.                                      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Insights into why these errors occurred**                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Based on the analysis above, it appears that the LLM model made mistakes in the following areas:                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Difficulty in detecting nuanced sentiment**: The model struggled to distinguish between mixed reviews and  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m genuinely positive ones.                                                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Sensitivity to strong language**: The use of words like \"hate\" and \"ridiculous\" led the model to           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m incorrectly classify texts as negative, even when they contained some positive sentiments.                      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3. **Inability to recognize sarcasm and irony**: The LLM model failed to pick up on the playful tone in the     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m second example, leading it to misclassify the text as negative.                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Strategies to improve the classification prompt**                                                             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m To reduce both false positives and false negatives, I recommend the following strategies:                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Improve sentiment analysis for nuanced reviews**: Modify the prompt to account for mixed reviews with both \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m positive and negative sentiments.                                                                               \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Sensitivity adjustments**: Fine-tune the model's sensitivity to strong language by considering context and \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m tone.                                                                                                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3. **Incorporate sarcasm and irony detection**: Implement techniques to identify when text contains sarcastic   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m or ironic statements, which can lead to incorrect classifications.                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 4. **Training data curation**: Update the training dataset with more diverse examples that showcase nuanced     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m sentiment, sarcasm, and irony.                                                                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 5. **Prompt modification**: Modify the classification prompt to explicitly ask for both positive and negative   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m sentiments, rather than relying solely on a single label.                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Recommendations**                                                                                             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Based on these strategies, I suggest modifying the classification prompt as follows:                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. **Classification Prompt**: Instead of asking \"Is this text positive (1) or negative (0)?\", modify it to:     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \"What sentiment does this text convey? (Positive, Negative, Mixed)\"                                             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. **Incorporate sentiment analysis for nuanced reviews**: Update the training data and model architecture to   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m better account for mixed reviews with both positive and negative sentiments.                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3. **Implement sarcasm and irony detection**: Integrate techniques to identify when text contains sarcastic or  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m ironic statements, which can lead to incorrect classifications.                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m By implementing these strategies and modifications, we can improve the LLM model's ability to accurately        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m classify binary sentiment texts, reducing false positives and false negatives.                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new prompt...\n",
      "\n",
      "Iteration 2/3\n",
      "Processing text 1/10\n",
      "Prediction 1/10: 0 | Ground Truth: 1 ❌ (FN)\n",
      "Processing text 2/10\n",
      "Prediction 2/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 3/10\n",
      "Prediction 3/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 4/10\n",
      "Prediction 4/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 5/10\n",
      "Prediction 5/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 6/10\n",
      "Prediction 6/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 7/10\n",
      "Prediction 7/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 8/10\n",
      "Prediction 8/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 9/10\n",
      "Prediction 9/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 10/10\n",
      "Prediction 10/10: 1 | Ground Truth: 1 ✅ (TP)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 2</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.8000 │\n",
       "│ Recall              │ 0.8000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8000 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 2\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.8000 │\n",
       "│ Recall              │ 0.8000 │\n",
       "│ Accuracy            │ 0.8000 │\n",
       "│ F1-score            │ 0.8000 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassifications...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">╭──────────────────────────────────────── Analysis of Misclassifications ─────────────────────────────────────────╮</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Analysis**                                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **False Negatives (Negative texts misclassified as Positive)**                                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. The first example is a critique of the movie \"Shuttle\" that uses strong language, such as \"I hate guns,\" but <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> then praises the film's ability to evoke emotions and its entertainment value. However, upon closer inspection, <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> the reviewer criticizes various aspects of the movie, including the clichéd plot, predictable surprises, and    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> annoying character revelations.                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. The second example is a scathing review of the movie \"Cliffhanger\" that highlights numerous issues with the  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> story, characters, dialog, and action sequences. Despite acknowledging some excitement in the film's airplane   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> hijack scene and praising its visuals, the reviewer calls the entire experience \"mindless fun at its best,\"     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> implying it's still not well-done.                                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **False Positives (Positive texts misclassified as Negative)**                                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1. The first example is a review that seems to pan the movie \"Cliffhanger\" with strong language about its bad   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> dialog, clichéd storylines, and silly plot twists. However, upon closer inspection, the reviewer uses phrases   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> like \"entertains through some suspense,\" \"good action-sequences,\" and \"nice snowy mountainous setting.\" This    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> suggests that while the reviewer is critical of various aspects, they still find value in certain elements.     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2. The second example appears to be a scathing critique of the movie's characters, story, and dialog, with      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> phrases like \"there is nothing in this movie to be taken seriously,\" and \"dumb that it's sometimes almost       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> hilarious.\" However, the reviewer also praises specific aspects, such as the snowy mountainous setting, some    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> suspense, good action-sequences, and unintentional humor.                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Insights**                                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> * The LLM model appears to have difficulty distinguishing between scathing criticism and sarcasm/humor.         <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> * Some reviewers use strong language to express their dissatisfaction with various movie elements while still   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> finding value in other aspects.                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> * In both cases, the reviewers' opinions seem to be nuanced, with multiple layers of criticism or praise that   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> the model failed to capture.                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Correct Classifications**                                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> * The first example should have been classified as **Negative (0)**, not Positive. While it does praise some    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> aspects of the movie, the criticisms far outweigh any positive comments.                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> * The second example should have been classified as **Positive (1)**, not Negative. Despite its harsh language  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> and criticism of various elements, the reviewer acknowledges that the film has some redeeming qualities.        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> **Strategies to Improve Classification Prompt**                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> To reduce both false positives and false negatives, I recommend:                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 1.  **More nuanced prompts**: Incorporate multiple layers or aspects into the classification prompt to account  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> for more complex opinions. This could include separate sections or questions for evaluating the reviewer's      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> overall sentiment (e.g., positive/negative) versus their specific praises/criticisms.                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 2.  **Clarify ambiguity**: Provide clear guidelines on how to handle ambiguous language, sarcasm, and humor in  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> reviews. For example, define what constitutes a \"positive\" review despite its negative tone or vice versa.      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 3.  **Contextual understanding**: Incorporate contextual understanding into the model's decision-making process <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> by considering factors like reviewer bias, personal preferences, and review style (e.g., scathing vs.           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> constructive criticism).                                                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 4.  **More advanced sentiment analysis**: Leverage more sophisticated natural language processing techniques to <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> better capture subtle nuances in opinions. This could involve integrating multiple sentiment analysis models or <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> techniques to provide a comprehensive understanding of the text.                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> 5.  **Human evaluation**: Regularly evaluate and fine-tune the classification prompt with human evaluators who  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> can assess the model's performance against a gold standard. This will help identify biases, errors, and areas   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> for improvement.                                                                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> By implementing these strategies, the model should be better equipped to handle nuanced reviews and improve its <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> overall accuracy in distinguishing between positive and negative opinions.                                      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m╭─\u001b[0m\u001b[1m───────────────────────────────────────\u001b[0m\u001b[1m Analysis of Misclassifications \u001b[0m\u001b[1m────────────────────────────────────────\u001b[0m\u001b[1m─╮\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Analysis**                                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **False Negatives (Negative texts misclassified as Positive)**                                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. The first example is a critique of the movie \"Shuttle\" that uses strong language, such as \"I hate guns,\" but \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m then praises the film's ability to evoke emotions and its entertainment value. However, upon closer inspection, \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m the reviewer criticizes various aspects of the movie, including the clichéd plot, predictable surprises, and    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m annoying character revelations.                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. The second example is a scathing review of the movie \"Cliffhanger\" that highlights numerous issues with the  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m story, characters, dialog, and action sequences. Despite acknowledging some excitement in the film's airplane   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m hijack scene and praising its visuals, the reviewer calls the entire experience \"mindless fun at its best,\"     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m implying it's still not well-done.                                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **False Positives (Positive texts misclassified as Negative)**                                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1. The first example is a review that seems to pan the movie \"Cliffhanger\" with strong language about its bad   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m dialog, clichéd storylines, and silly plot twists. However, upon closer inspection, the reviewer uses phrases   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m like \"entertains through some suspense,\" \"good action-sequences,\" and \"nice snowy mountainous setting.\" This    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m suggests that while the reviewer is critical of various aspects, they still find value in certain elements.     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2. The second example appears to be a scathing critique of the movie's characters, story, and dialog, with      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m phrases like \"there is nothing in this movie to be taken seriously,\" and \"dumb that it's sometimes almost       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m hilarious.\" However, the reviewer also praises specific aspects, such as the snowy mountainous setting, some    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m suspense, good action-sequences, and unintentional humor.                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Insights**                                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m * The LLM model appears to have difficulty distinguishing between scathing criticism and sarcasm/humor.         \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m * Some reviewers use strong language to express their dissatisfaction with various movie elements while still   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m finding value in other aspects.                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m * In both cases, the reviewers' opinions seem to be nuanced, with multiple layers of criticism or praise that   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m the model failed to capture.                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Correct Classifications**                                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m * The first example should have been classified as **Negative (0)**, not Positive. While it does praise some    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m aspects of the movie, the criticisms far outweigh any positive comments.                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m * The second example should have been classified as **Positive (1)**, not Negative. Despite its harsh language  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m and criticism of various elements, the reviewer acknowledges that the film has some redeeming qualities.        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m **Strategies to Improve Classification Prompt**                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m To reduce both false positives and false negatives, I recommend:                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 1.  **More nuanced prompts**: Incorporate multiple layers or aspects into the classification prompt to account  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m for more complex opinions. This could include separate sections or questions for evaluating the reviewer's      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m overall sentiment (e.g., positive/negative) versus their specific praises/criticisms.                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 2.  **Clarify ambiguity**: Provide clear guidelines on how to handle ambiguous language, sarcasm, and humor in  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m reviews. For example, define what constitutes a \"positive\" review despite its negative tone or vice versa.      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 3.  **Contextual understanding**: Incorporate contextual understanding into the model's decision-making process \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m by considering factors like reviewer bias, personal preferences, and review style (e.g., scathing vs.           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m constructive criticism).                                                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 4.  **More advanced sentiment analysis**: Leverage more sophisticated natural language processing techniques to \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m better capture subtle nuances in opinions. This could involve integrating multiple sentiment analysis models or \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m techniques to provide a comprehensive understanding of the text.                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m 5.  **Human evaluation**: Regularly evaluate and fine-tune the classification prompt with human evaluators who  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m can assess the model's performance against a gold standard. This will help identify biases, errors, and areas   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m for improvement.                                                                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m By implementing these strategies, the model should be better equipped to handle nuanced reviews and improve its \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m overall accuracy in distinguishing between positive and negative opinions.                                      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new prompt...\n",
      "\n",
      "Iteration 3/3\n",
      "Processing text 1/10\n",
      "Prediction 1/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 2/10\n",
      "Prediction 2/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 3/10\n",
      "Prediction 3/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 4/10\n",
      "Prediction 4/10: 1 | Ground Truth: 0 ❌ (FP)\n",
      "Processing text 5/10\n",
      "Prediction 5/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 6/10\n",
      "Prediction 6/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 7/10\n",
      "Prediction 7/10: 1 | Ground Truth: 1 ✅ (TP)\n",
      "Processing text 8/10\n",
      "Prediction 8/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 9/10\n",
      "Prediction 9/10: 0 | Ground Truth: 0 ✅ (TN)\n",
      "Processing text 10/10\n",
      "Prediction 10/10: 1 | Ground Truth: 1 ✅ (TP)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Evaluation Metrics - Iteration 3</span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">  Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.8333 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.9000 │\n",
       "│ F1-score            │ 0.9091 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mEvaluation Metrics - Iteration 3\u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Value\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Precision           │ 0.8333 │\n",
       "│ Recall              │ 1.0000 │\n",
       "│ Accuracy            │ 0.9000 │\n",
       "│ F1-score            │ 0.9091 │\n",
       "│ Invalid Predictions │      0 │\n",
       "└─────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Best Prompt:</span> │\n",
       "╰──────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────╮\n",
       "│ \u001b[1;32mBest Prompt:\u001b[0m │\n",
       "╰──────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here's a revised prompt that incorporates the strategies outlined earlier:\n",
       "\n",
       "\"I'm trying to understand the sentiment behind this text. Please classify it as Positive <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span> if it expresses more \n",
       "praise than criticism, Negative <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span> if it expresses more criticism than praise, and Mixed <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"font-weight: bold\">)</span> if it contains both \n",
       "positive and negative sentiments with no clear majority.\n",
       "\n",
       "When evaluating sentiment, please consider the reviewer's overall opinion, taking into account both explicit \n",
       "language and implied tone. Be cautious of strong language, sarcasm, and irony - if you're unsure, choose <span style=\"color: #008000; text-decoration-color: #008000\">'Mixed'</span>. \n",
       "Also, acknowledge that some reviews may be nuanced or balanced, praising certain aspects while criticizing others.\n",
       "\n",
       "Consider factors like reviewer bias, personal preferences, and review style when making your decision. If the text \n",
       "uses humor, irony, or sarcasm to express a negative opinion, classify it as Negative <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>. Conversely, if the text \n",
       "uses strong language but still conveys a positive overall sentiment, classify it as Positive <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "For cases where the text is balanced or nuanced, with both praise and criticism being significant, choose Mixed \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"font-weight: bold\">)</span>. This will help you accurately capture the complexity of real-world opinions.\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here's a revised prompt that incorporates the strategies outlined earlier:\n",
       "\n",
       "\"I'm trying to understand the sentiment behind this text. Please classify it as Positive \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m if it expresses more \n",
       "praise than criticism, Negative \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m if it expresses more criticism than praise, and Mixed \u001b[1m(\u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1m)\u001b[0m if it contains both \n",
       "positive and negative sentiments with no clear majority.\n",
       "\n",
       "When evaluating sentiment, please consider the reviewer's overall opinion, taking into account both explicit \n",
       "language and implied tone. Be cautious of strong language, sarcasm, and irony - if you're unsure, choose \u001b[32m'Mixed'\u001b[0m. \n",
       "Also, acknowledge that some reviews may be nuanced or balanced, praising certain aspects while criticizing others.\n",
       "\n",
       "Consider factors like reviewer bias, personal preferences, and review style when making your decision. If the text \n",
       "uses humor, irony, or sarcasm to express a negative opinion, classify it as Negative \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m. Conversely, if the text \n",
       "uses strong language but still conveys a positive overall sentiment, classify it as Positive \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m.\n",
       "\n",
       "For cases where the text is balanced or nuanced, with both praise and criticism being significant, choose Mixed \n",
       "\u001b[1m(\u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1m)\u001b[0m. This will help you accurately capture the complexity of real-world opinions.\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Output Format:</span> │\n",
       "╰────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────╮\n",
       "│ \u001b[1;32mOutput Format:\u001b[0m │\n",
       "╰────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are to act as a binary responder. For every question asked, reply strictly with <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span> for positive or <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span> for \n",
       "negative. Do NOT include any additional text or explanation.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are to act as a binary responder. For every question asked, reply strictly with \u001b[32m'1'\u001b[0m for positive or \u001b[32m'0'\u001b[0m for \n",
       "negative. Do NOT include any additional text or explanation.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                         Comparison of All Iterations                         </span>\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Iteration </span>┃<span style=\"font-weight: bold\"> Precision </span>┃<span style=\"font-weight: bold\"> Recall </span>┃<span style=\"font-weight: bold\"> Accuracy </span>┃<span style=\"font-weight: bold\"> F1-score </span>┃<span style=\"font-weight: bold\"> Invalid Predictions </span>┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│     1     │    0.7143 │ <span style=\"font-weight: bold\">1.0000</span> │   0.8000 │   0.8333 │              <span style=\"font-weight: bold\">0.0000</span> │\n",
       "│     2     │    0.8000 │ 0.8000 │   0.8000 │   0.8000 │              <span style=\"font-weight: bold\">0.0000</span> │\n",
       "│     3     │    <span style=\"font-weight: bold\">0.8333</span> │ <span style=\"font-weight: bold\">1.0000</span> │   <span style=\"font-weight: bold\">0.9000</span> │   <span style=\"font-weight: bold\">0.9091</span> │              <span style=\"font-weight: bold\">0.0000</span> │\n",
       "└───────────┴───────────┴────────┴──────────┴──────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                         Comparison of All Iterations                         \u001b[0m\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mIteration\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPrecision\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRecall\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAccuracy\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mF1-score\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInvalid Predictions\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│     1     │    0.7143 │ \u001b[1m1.0000\u001b[0m │   0.8000 │   0.8333 │              \u001b[1m0.0000\u001b[0m │\n",
       "│     2     │    0.8000 │ 0.8000 │   0.8000 │   0.8000 │              \u001b[1m0.0000\u001b[0m │\n",
       "│     3     │    \u001b[1m0.8333\u001b[0m │ \u001b[1m1.0000\u001b[0m │   \u001b[1m0.9000\u001b[0m │   \u001b[1m0.9091\u001b[0m │              \u001b[1m0.0000\u001b[0m │\n",
       "└───────────┴───────────┴────────┴──────────┴──────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All logs saved in directory: /Users/danielfiuzadosil/Documents/GitHub/AI-Prompt-Optimiser/runs/prompt_optimization_logs_20240925_155155\n",
      "\n",
      "Best Prompt:\n",
      "Here's a revised prompt that incorporates the strategies outlined earlier:\n",
      "\n",
      "\"I'm trying to understand the sentiment behind this text. Please classify it as Positive (1) if it expresses more praise than criticism, Negative (0) if it expresses more criticism than praise, and Mixed (-1) if it contains both positive and negative sentiments with no clear majority.\n",
      "\n",
      "When evaluating sentiment, please consider the reviewer's overall opinion, taking into account both explicit language and implied tone. Be cautious of strong language, sarcasm, and irony - if you're unsure, choose 'Mixed'. Also, acknowledge that some reviews may be nuanced or balanced, praising certain aspects while criticizing others.\n",
      "\n",
      "Consider factors like reviewer bias, personal preferences, and review style when making your decision. If the text uses humor, irony, or sarcasm to express a negative opinion, classify it as Negative (0). Conversely, if the text uses strong language but still conveys a positive overall sentiment, classify it as Positive (1).\n",
      "\n",
      "For cases where the text is balanced or nuanced, with both praise and criticism being significant, choose Mixed (-1). This will help you accurately capture the complexity of real-world opinions.\"\n",
      "\n",
      "Best Metrics:\n",
      "{'precision': 0.8333333333333334, 'recall': 1.0, 'accuracy': 0.9, 'f1': 0.9090909090909091, 'predictions': [1, 0, 1, 1, 1, 0, 1, 0, 0, 1], 'false_positives': [{'text': 'I hate guns and have never murdered anyone, but when even half of the events that take place in \\'Shuttle\\' happen to you or close ones and you find a gun, YOU SHOOT YOUR ATTACKER. THREE TIMES. FIVE TIMES. Whatever makes the pulse stop on them and increase on you. I think even God would say, \"Good call.\" In a very \\'Hostel\\'-type film, but more realistic Â? as this really could happen to anyone, well, if you\\'re a pretty young woman, that is Â? \\'Shuttle\\' was a decent film, though on the long side. A few good shocks (always call AAA even just to change a tire), basically just one surprise but for the most part, you could see things coming. And aside from the typical \"tie him up\" instead of the previously mentioned shooting him, the most annoying part was the revelation towards the closing from one best friend to the other. Getting past those, it\\'s enjoyable for what it is. Basically, we have two unsuspecting females traveling alone from Mexico home (wow, that\\'s original) and one lost her luggage preventing them from leaving the airport until late. And after an obvious foreshadowed sign-language scene, they enter a \"too-good-to-be-true\" half-price shuttle ride. ClichÃ©d jocks, previously introduced, con their way on the shuttle, to join what appears to be Alan Ruck\\'s stunt double from \\'Speed.\\' From here, it\\'s obvious what happens (I did mention it was a \\'Hostel\\' knock off) but still, I didn\\'t find too much horrible, yet nothing spectacular. Though, it would\\'ve served the audience better with roughly 15-20 minutes deleted, I would recommend if you have almost 2 hours to kill and are into sick horror.', 'label': 0}], 'false_negatives': [], 'invalid_predictions': 0}\n"
     ]
    }
   ],
   "source": [
    "# Run the prompt optimization process\n",
    "best_prompt, best_metrics = optimize_prompt(\n",
    "    initial_prompt,\n",
    "    output_format_prompt,\n",
    "    eval_data,\n",
    "    iterations,\n",
    "    eval_provider=eval_provider,\n",
    "    eval_model=eval_model,\n",
    "    optim_provider=optim_provider,\n",
    "    optim_model=optim_model,\n",
    "    output_schema=output_schema\n",
    ")\n",
    "# After running the optimization process, you can analyze the results by checking \n",
    "# the generated log files in the `runs/prompt_optimization_logs_YYYYMMDD_HHMMSS` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
